{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNyfW5el9FB7"
   },
   "source": [
    "### The German Traffic Sign Benchmark\n",
    "\n",
    "Student Name 1: Panagiotis Michalopoulos\n",
    "\n",
    "Student Name 2: Filip Finfando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2t23RDU9FB9"
   },
   "source": [
    "Download full data set from http://benchmark.ini.rub.de/?section=gtsdb&subsection=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pt81Eft39FCF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use those numbers to change range in the next cell\n",
    "! wc -l ./data/bin_labels_sorted.txt\n",
    "! grep \"00600.ppm\" -B 5 -A 5 -m 1 ./data/bin_labels_sorted.txt --line-number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pt81Eft39FCF"
   },
   "outputs": [],
   "source": [
    "number_of_samples = 148522\n",
    "split_at = 97654\n",
    "train_samples = 90000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pt81Eft39FCF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "IMG_HEIGHT = 600\n",
    "#SIGN_SIZE = (224, 224)\n",
    "SIGN_SIZE = (32, 32)\n",
    "\n",
    "# Function for reading the images\n",
    "def readImages(rootpath, images_range, signs_range):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "    Arguments: path to the traffic sign data, for example 'FullIJCNN2013'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = {} # original image\n",
    "    scales = {} # original scale\n",
    "    for num in images_range:\n",
    "        filename = rootpath + '/' + \"{:05d}\".format(num) + '.ppm'\n",
    "        img = cv2.imread(filename, cv2.IMREAD_COLOR)\n",
    "        scale = IMG_HEIGHT / float(img.shape[0])\n",
    "        img_resized = cv2.resize(img, (int(img.shape[1]*scale),int(img.shape[0]*scale)))\n",
    "        images.setdefault(filename,[]).append(img_resized)\n",
    "        scales.setdefault(filename,[]).append(scale)\n",
    "\n",
    "    files = [] # filenames\n",
    "    signs = [] # traffic sign image\n",
    "    bboxes = [] # corresponding box detection\n",
    "    labels = [] # traffic sign type\n",
    "    data = np.genfromtxt('./data/bin_labels_sorted.txt', delimiter=';', dtype=str, usecols=range(0, 6))\n",
    "    for elem in signs_range:\n",
    "        filename = rootpath + '/' + data[elem][0]\n",
    "        img = images.get(filename)[0]\n",
    "        scale = scales.get(filename)[0]\n",
    "        bbox = np.array([int(data[elem][1]), int(data[elem][2]), int(data[elem][3]), int(data[elem][4])]) * scale\n",
    "        sign = img[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "        sign_resized = cv2.resize(sign, SIGN_SIZE)\n",
    "        files.append(filename)\n",
    "        signs.append(sign_resized)\n",
    "        bboxes.append(bbox)\n",
    "        labels.append(data[elem][5])\n",
    "    return images, files, signs, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugfSjhY69FCH"
   },
   "outputs": [],
   "source": [
    "# The German Traffic Sign Recognition Benchmark\n",
    "train_images, train_files, train_signs, train_bboxes, train_labels = readImages('../FullIJCNN2013', range(0,600), range(0,split_at))\n",
    "test_images, test_files, test_signs, test_bboxes, test_labels = readImages('../FullIJCNN2013', range(600,900), range(split_at,number_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "zxI01qsB9FCJ",
    "outputId": "1400a5ba-fbdc-4716-fe37-0bdb1bca2745",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Show examples from each class\n",
    "class_names = np.unique(train_labels)\n",
    "num_classes = len(class_names)\n",
    "print(class_names)\n",
    "\n",
    "for r in range(500,510):\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    for i in range(num_classes):\n",
    "        ax = fig.add_subplot(6, 9, 1 + i, xticks=[], yticks=[])\n",
    "        ax.set_title(class_names[i])\n",
    "        indices = np.where(np.isin(train_labels, class_names[i]))[0]\n",
    "        plt.imshow(cv2.cvtColor(train_signs[int(np.random.choice(indices, 1))], cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEpCoIRY9FCM"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_files, train_signs, train_bboxes, train_labels = shuffle(train_files, train_signs, train_bboxes, train_labels)\n",
    "# plt.imshow(cv2.cvtColor(train_images.get(train_files[0])[0], cv2.COLOR_BGR2RGB))\n",
    "# plt.show()\n",
    "# plt.imshow(cv2.cvtColor(train_signs[0], cv2.COLOR_BGR2RGB))\n",
    "# plt.show()\n",
    "# print(train_bboxes[0])\n",
    "# print(train_labels[0])\n",
    "\n",
    "# Data pre-processing\n",
    "tr_signs = np.array(train_signs)[0:train_samples]\n",
    "tr_labels = np.array(train_labels)[0:train_samples]\n",
    "va_signs = np.array(train_signs)[train_samples:split_at]\n",
    "va_labels = np.array(train_labels)[train_samples:split_at]\n",
    "te_signs = np.array(test_signs)\n",
    "te_labels = np.array(test_labels)\n",
    "\n",
    "tr_signs = tr_signs.astype('float32')\n",
    "va_signs = va_signs.astype('float32')\n",
    "te_signs = te_signs.astype('float32')\n",
    "tr_signs /= 255.0\n",
    "va_signs /= 255.0\n",
    "te_signs /= 255.0\n",
    "\n",
    "from keras.utils import np_utils\n",
    "tr_labels = np_utils.to_categorical(tr_labels, num_classes)\n",
    "va_labels = np_utils.to_categorical(va_labels, num_classes)\n",
    "te_labels = np_utils.to_categorical(te_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK9aqvC39FCP"
   },
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUELCOIA9FCT"
   },
   "source": [
    "## Assignment 3.2: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "u3Dpv_oU9FCU",
    "outputId": "9d9705c8-2b42-4c83-b03b-da2a7c33ab69"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "cnn = Sequential()\n",
    "## If You preprocessed with gray scaling and local histogram equivalization then input_shape = (32,32,1) else (32,32,3)\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=(SIGN_SIZE[0], SIGN_SIZE[1], 3)))\n",
    "#cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(BatchNormalization())\n",
    "cnn.add(Dropout(0.5))\n",
    "#cnn.add(Flatten())\n",
    "cnn.add(GlobalAveragePooling2D())\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dropout(0.7))\n",
    "cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "cnn.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3708
    },
    "colab_type": "code",
    "id": "WoVkm0869FCX",
    "outputId": "a64679d0-ec8c-4323-e7ea-a8ff6317c9be"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "class_weight = {0: 0.1,\n",
    "                1: 0.9,\n",
    "               }\n",
    "\n",
    "data = cnn.fit(\n",
    "    tr_signs, tr_labels, \n",
    "    batch_size=128, epochs=20, verbose=2, \n",
    "    validation_data=(va_signs, va_labels), \n",
    "    callbacks=[tensorboard, early_stopping], \n",
    "    class_weight=class_weight,\n",
    ")\n",
    "\n",
    "start = time()\n",
    "loss, acc = cnn.evaluate(te_signs, te_labels, verbose=0)\n",
    "end = time()\n",
    "print('CNN took ' + str(end - start) + ' seconds')\n",
    "print('Test loss: ' + str(loss) + ' - Accuracy: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "id": "oEdZqsULmP36",
    "outputId": "4e028867-4665-4089-f8bf-7516f5e0e03a"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(data.history['acc'])\n",
    "plt.plot(data.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(data.history['loss'])\n",
    "plt.plot(data.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4118
    },
    "colab_type": "code",
    "id": "IDCHmLFS9FCb",
    "outputId": "634eb0d9-5b52-475f-b5ad-e8e241960b89"
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    np.argmax(te_labels, axis=1), \n",
    "    np.argmax(cnn.predict(te_signs), axis=1), \n",
    "    classes=np.array([0,1]),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "# plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4118
    },
    "colab_type": "code",
    "id": "IDCHmLFS9FCb",
    "outputId": "634eb0d9-5b52-475f-b5ad-e8e241960b89"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/test_pred.pickle', 'rb') as fp:\n",
    "    test_pred = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4118
    },
    "colab_type": "code",
    "id": "IDCHmLFS9FCb",
    "outputId": "634eb0d9-5b52-475f-b5ad-e8e241960b89",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run some tests\n",
    "i=0\n",
    "\n",
    "number_of_images = 15\n",
    "number_of_boxes_to_draw = 1000\n",
    "\n",
    "for filename in test_images:\n",
    "    # Draw predictions\n",
    "    aux = test_images.get(filename)[0].copy()\n",
    "    for bbox in test_pred.get(filename)[0][:number_of_boxes_to_draw]:  \n",
    "        \n",
    "        #filter some boxes before making predictions\n",
    "        dx = int(bbox[0])-int(bbox[2])\n",
    "        dy = int(bbox[1])-int(bbox[3])\n",
    "        ratio = abs(dy/dx)\n",
    "        if ratio<1:\n",
    "            ratio=1/ratio\n",
    "        if ratio>1.2:\n",
    "            continue\n",
    "        \n",
    "        # make a prediction\n",
    "        roi = test_images.get(filename)[0][int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "        SIGN_SIZE_CNN = (32, 32)\n",
    "        roi_resized = cv2.resize(roi, SIGN_SIZE_CNN)\n",
    "        roi_resized = roi_resized.astype('float32')\n",
    "        roi_resized /= 255.0\n",
    "        roi_resized = np.reshape(roi_resized, [1,SIGN_SIZE_CNN[0],SIGN_SIZE_CNN[1],3])\n",
    "        \n",
    "        confidence = cnn.predict(roi_resized)\n",
    "        \n",
    "        # print rectangle if confidence is large\n",
    "        # those are boxes that we think have the largest probability to have a traffic sign inside\n",
    "#         dx = int(bbox[0])-int(bbox[2])\n",
    "#         dy = int(bbox[1])-int(bbox[3])\n",
    "#         ratio = abs(dy/dx)\n",
    "#         if ratio<1:\n",
    "#             ratio=1/ratio\n",
    "        if confidence[0][1] > 0.9:\n",
    "            cv2.rectangle(aux, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])), (0,255,0), 3)\n",
    "\n",
    "        # print contents of box if you like\n",
    "#         not_a_sign = aux[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "#         plt.imshow(cv2.cvtColor(not_a_sign, cv2.COLOR_BGR2RGB))\n",
    "#         plt.show()\n",
    "            \n",
    "\n",
    "    \n",
    "    # Draw ground truth\n",
    "    # this is what we should detect\n",
    "    if filename in test_files:\n",
    "        for idx in [i for i, x in enumerate(test_files) if x == filename]:\n",
    "            bbox = test_bboxes[idx]\n",
    "#             cv2.rectangle(aux, (int(bbox[0]),int(bbox[1])), (int(bbox[2]),int(bbox[3])), (0,0,255), 3)\n",
    "            \n",
    "            # print contents of box if you like\n",
    "#             sign = aux[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n",
    "#             plt.imshow(cv2.cvtColor(sign, cv2.COLOR_BGR2RGB))\n",
    "#             plt.show()\n",
    "    \n",
    "    plt.imshow(cv2.cvtColor(aux, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    i+=1\n",
    "    if i==number_of_images: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4118
    },
    "colab_type": "code",
    "id": "IDCHmLFS9FCb",
    "outputId": "634eb0d9-5b52-475f-b5ad-e8e241960b89"
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = cnn.to_json()\n",
    "with open(\"./models/bin_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "cnn.save_weights(\"./models/bin_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZNyfW5el9FB7"
   ],
   "name": "Assignment_3_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
